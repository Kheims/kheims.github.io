---
layout: post
title: "Distributed Data Parallelism, a gentle intro"
date: 2025-09-12 10:00:00 +0200
categories: [distributed, pytorch, llms]
math: true
---

# Distributed Data Parallelism

As LLM grow larger, training them efficiently requires strategies that go beyond a single GPU. Parallelism provides the key: by splitting either the data or the model itself across multiple devices, we can reduce training time and scale to models with billions of parameters. Three main approaches are widely used:

1. **Distributed Data Parallelism (DDP)**: the most common technique, where each GPU holds a full copy of the model and processes a different slice of the training data.
2. **Pipeline Parallelism (PP)**: the model is partitioned into sequential stages, and *micro-batches* of data are passed through the pipeline so that all GPUs remain busy.
3. **Tensor Parallelism (TP)**: individual matrix operations inside a layer are split across GPUs, allowing training of layers too large to fit on a single device.

In this lesson, we will focus on **Distributed Data Parallelism (DDP)**, since it is the foundation of most large-scale training setups and is often the first step toward scaling deep learning workloads.

The model used is : HuggingFaceTB/SmolLM2-360M-Instruct ( https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct ) 

and we are using the mrpc dataset from glue ( https://huggingface.co/datasets/nyu-mll/glue/viewer/mrpc?views%5B%5D=mrpc_train) 

In the DDP configuration, all models across all GPUs are the same, they are just dealing with different mini batches of the global batch and after each iteration of the training all the models remain with the same parameters. 

![DDP.drawio.png](/assets/img/ddp/DDP.drawio.png)

Specifically, each model in it’s forward pass and backward pass treat it’s own batch of data and before doing the optimisation step first accumulate all the gradients from all the models. There is only a single communication point between the gpus, making it effective in terms of bandwidth usage.

So to summarize the process is as follows : 

$B_i = \frac{B}{n}$                Each replica gets a mini-batch of size B/n, in our example that is 128 / 4 = 32

$g = \frac{1}{n}\sum_{i=1}^ng_i$   After the forward and backward pass the gradients from all replicas are averaged

 

$\theta_i = \theta_i - g$     After the gradient accumulation, each replica updates its own copy of the parameters using the accumulated gradient.

The figure below summarize the process of DDP. 

![DDP_process.drawio.png](/assets/img/ddp/DDP_process.drawio.png)

The custom class of DDP is :  

```python
class SimpleDistributedDataParallelism:
    def __init__(self, model:torch.nn.Module):
        self.model = model

        for param in model.parameters():
            rank0_param = param.data.clone()
            dist.broadcast(rank0_param, src=0)
            if not torch.equal(param.data, rank0_param):
                raise ValueError(
                    "Expected model parameters to be identical during `__init__`, but this is not true. "
                    "Make sure to set global common seeds before creating your model"
                )

    def sync_gradients(self):
        for param in self.model.parameters():
            if param.grad is not None:
                dist.all_reduce(param.grad, op=dist.ReduceOp.SUM) # aggregate across all ranks
                param.grad /= dist.get_world_size() # average
    
    def __call__(self, *args, **kwargs):
        return self.model(*args, **kwargs)
    
    def train(self):
        self.model.train()
    
    def eval(self):
        self.model.eval()
```

As we can see, in the initialization of the SimpleDDP object, we need to store the model ( this is done in each gpu, where each gpu has it’s own copy of the model according to the setup of the training environment ), we also need to make sure that the parameters are the same across all gpus otherwise we raise a ValueError. 

the call, train and eval methods are simple wrapper that are self describing. 

The heart of the Distributed Data Parallelism lies in the sync_gradients method, this method is responsible of synchronizing between the gradients after the forward and backward pass, effectively allowing for a kind of parallelized training across multiple batches of data.

Let’s make a comparison between running an epoch consisting of a batch size of 16 on a single GPU  and running an epoch consisting of a batch size of 8 on 2 gpus by using DDP : 

```python
%%rank [0]

train_ds = dataset.shuffle(seed=42)
def collate_func(batch):
    return tokenizer.pad(
        batch,
        padding="longest",
        max_length=None,
        pad_to_multiple_of=8,
        return_tensors="pt",
    )
per_device_batch_size = 16
train_dataloader = DataLoader(
    train_ds,
    batch_size=per_device_batch_size,
    collate_fn=collate_func,
    drop_last=True,
    shuffle=True
)
model = get_smol_model()
model.to(device)
optimizer = torch.optim.SGD(model.model.parameters(), lr=1e-3)

start_time = time.time()
num_batches = 0
for (i, batch) in enumerate(train_dataloader):
    if i > 20:
        break
    batch = {k: v.to(device) for k, v in batch.items()}
    
    torch.cuda.synchronize()  
    batch_start = time.time()
    
    output = model(**batch)
    output.loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    
    torch.cuda.synchronize() 
    num_batches += 1

torch.cuda.synchronize()  # Ensure all GPU ops are done
total_time = time.time() - start_time
avg_time_per_batch = total_time / num_batches
print(f"Total training time: {total_time:.2f} seconds")
print(f"Average time per batch: {avg_time_per_batch:.4f} seconds")
```

![image.png](/assets/img/ddp/image.png)

Now for the DDP :

We first need to shard the data for each rank : 

```python
# do initial shuffle
train_ds = dataset.shuffle(seed=42)

# Shard data for first parallel dimension across ranks
ds_length = len(train_ds)
ds_length_per_rank = ds_length // get("ws")
rank = get("rank")
start = rank * ds_length_per_rank
end = start + ds_length_per_rank if rank != get("ws") - 1 else ds_length

# Each process calls the same line of code independently, but because rank 
# is different, each ends up with a different subset.
train_shard = train_ds.select(list(range(start, end)))
```

Then we can make our training loop : 

```python
train_ds = dataset.shuffle(seed=42)

def collate_func(batch):
    return tokenizer.pad(
        batch,
        padding="longest",
        max_length=None,
        pad_to_multiple_of=8,
        return_tensors="pt",
    )

per_device_batch_size = 16

train_dataloader = DataLoader(
    train_shard,
    batch_size=per_device_batch_size,
    collate_fn=collate_func,
    drop_last=True,
    shuffle=True
)

model = get_smol_model()
model.to(device)
optimizer = torch.optim.SGD(model.model.parameters(), lr=1e-3)

start_time = time.time()
num_batches = 0
for (i, batch) in enumerate(train_dataloader):
    if i > 20:
        break

    batch = {k: v.to(device) for k, v in batch.items()}
    
    torch.cuda.synchronize()  
    batch_start = time.time()
    
    output = model(**batch)
    output.loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    
    torch.cuda.synchronize()  
    num_batches += 1

torch.cuda.synchronize()  
total_time = time.time() - start_time
avg_time_per_batch = total_time / num_batches
print(f"Total training time: {total_time:.2f} seconds")
print(f"Average time per batch: {avg_time_per_batch:.4f} seconds")
```

![image.png](/assets/img/ddp/image%201.png)

Our comparison highlights a key strength of Distributed Data Parallelism. When moving from a single GPU to two GPUs, we doubled the global batch size (from 16 to 32) by keeping the per-device batch size fixed at 16. Despite processing twice as many samples per iteration, the wall-clock time per iteration remained essentially unchanged. So in practice, the cost of synchronizing gradients across devices can be negligible compared to the time spent on forward and backward computation.
